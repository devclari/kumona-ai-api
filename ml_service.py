import os
import numpy as np
# Importar configura√ß√µes do TensorFlow primeiro
from tf_config import configure_tensorflow_for_cloud_run, get_tensorflow_info, optimize_model_for_inference

import tensorflow as tf
from keras.models import load_model
import gdown
from PIL import Image
import logging
from typing import Tuple, Dict, Optional
import time

# Importar MLFlow
from mlflow_config import mlflow_manager
from mlflow_utils import performance_monitor

# Importar downloader de modelos
from model_downloader import model_downloader

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MLService:
    """Servi√ßo para carregar modelo e fazer predi√ß√µes"""
    
    def __init__(self):
        self.model = None
        self.class_names = ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']
        self.model_path = "best_model.keras"
        self.model_url = "https://drive.google.com/uc?id=1vSIfD3viT5JSxpG4asA8APCwK0JK9Dvu"
        self.is_loaded = False
        self.model_source = "local"  # "local" ou "mlflow"
        self.model_version = None

        # Log das configura√ß√µes do TensorFlow
        tf_info = get_tensorflow_info()
        logger.info(f"TensorFlow configuration: {tf_info}")

        # Configurar MLFlow
        self._setup_mlflow_integration()

    def _setup_mlflow_integration(self):
        """Configura integra√ß√£o com MLFlow"""
        try:
            # Verificar se deve usar MLFlow registry
            if mlflow_manager.config.ENABLE_MODEL_REGISTRY:
                logger.info("üîÑ MLFlow Model Registry habilitado")
                self.model_source = "mlflow"
            else:
                logger.info("üìÅ Usando modelo local")
                self.model_source = "local"

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na configura√ß√£o MLFlow, usando modelo local: {str(e)}")
            self.model_source = "local"

    def _load_model_from_mlflow(self) -> bool:
        """Carrega modelo do MLFlow registry"""
        try:
            logger.info("üîÑ Tentando carregar modelo do MLFlow registry...")

            # Carregar modelo do registry
            model = mlflow_manager.load_model()

            if model is not None:
                self.model = model
                self.model_version = mlflow_manager.config.MODEL_VERSION or "latest"
                logger.info(f"‚úÖ Modelo carregado do MLFlow registry (vers√£o: {self.model_version})")

                # Log par√¢metros do modelo no MLFlow
                mlflow_manager.log_params({
                    "model_source": "mlflow_registry",
                    "model_name": mlflow_manager.config.MLFLOW_MODEL_NAME,
                    "model_stage": mlflow_manager.config.MODEL_STAGE,
                    "model_version": self.model_version,
                    "class_names": str(self.class_names)
                })

                return True
            else:
                logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel carregar do MLFlow, tentando modelo local...")
                return False

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo do MLFlow: {str(e)}")
            return False

    def _load_model_local(self) -> bool:
        """Carrega modelo local"""
        try:
            if not self.download_model():
                return False

            logger.info("‚úÖ Carregando modelo local...")
            self.model = load_model(self.model_path)

            # Otimizar modelo para infer√™ncia
            self.model = optimize_model_for_inference(self.model)

            # Log par√¢metros do modelo no MLFlow
            mlflow_manager.log_params({
                "model_source": "local_file",
                "model_path": self.model_path,
                "model_url": self.model_url,
                "class_names": str(self.class_names)
            })

            logger.info("‚úÖ Modelo local carregado e otimizado com sucesso")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo local: {str(e)}")
            return False

    def download_model(self) -> bool:
        """Baixa o modelo usando o downloader multi-fonte"""
        try:
            return model_downloader.download_model()
        except Exception as e:
            logger.error(f"‚ùå Error downloading model: {str(e)}")
            return False
    
    def load_model(self) -> bool:
        """Carrega o modelo (MLFlow ou local)"""
        start_time = time.time()

        try:
            success = False

            # Tentar carregar do MLFlow primeiro se habilitado
            if self.model_source == "mlflow":
                success = self._load_model_from_mlflow()

                # Se falhar, tentar modelo local como fallback
                if not success:
                    logger.info("üîÑ Fallback para modelo local...")
                    success = self._load_model_local()
            else:
                # Carregar modelo local diretamente
                success = self._load_model_local()

            if success:
                self.is_loaded = True
                load_time = time.time() - start_time

                # Log m√©tricas de carregamento no MLFlow
                mlflow_manager.log_metrics({
                    "model_load_time_seconds": load_time,
                    "model_load_success": 1
                })

                logger.info(f"‚úÖ Modelo carregado com sucesso em {load_time:.2f}s")
                return True
            else:
                self.is_loaded = False
                load_time = time.time() - start_time

                # Log falha no carregamento
                mlflow_manager.log_metrics({
                    "model_load_time_seconds": load_time,
                    "model_load_success": 0
                })

                logger.error("‚ùå Falha ao carregar modelo")
                return False

        except Exception as e:
            self.is_loaded = False
            load_time = time.time() - start_time

            # Log erro no carregamento
            mlflow_manager.log_metrics({
                "model_load_time_seconds": load_time,
                "model_load_success": 0
            })

            mlflow_manager.log_params({
                "model_load_error": str(e)
            })

            logger.error(f"‚ùå Erro ao carregar modelo: {str(e)}")
            return False
    
    def preprocess_image(self, image: Image.Image) -> np.ndarray:
        """Preprocessa a imagem para predi√ß√£o"""
        try:
            # Redimensionar para 256x256
            img = image.resize((256, 256))
            
            # Converter para array numpy
            img_array = tf.keras.utils.img_to_array(img)
            
            # Expandir dimens√µes para batch
            img_array = np.expand_dims(img_array, axis=0).astype(np.float32)
            
            return img_array
        except Exception as e:
            logger.error(f"‚ùå Error preprocessing image: {str(e)}")
            raise
    
    def predict(self, image: Image.Image) -> Tuple[str, float, Dict[str, float]]:
        """
        Faz predi√ß√£o na imagem com tracking MLFlow

        Args:
            image: Imagem PIL para classifica√ß√£o

        Returns:
            Tuple contendo (classe_predita, confian√ßa, todas_predi√ß√µes)
        """
        start_time = time.time()

        try:
            if not self.is_loaded:
                raise ValueError("Model not loaded")

            # Preprocessar imagem
            img_array = self.preprocess_image(image)

            # Fazer predi√ß√£o
            predictions = self.model.predict(img_array, verbose=0)

            # Obter classe predita
            predicted_class_idx = np.argmax(predictions[0])
            predicted_class = self.class_names[predicted_class_idx]

            # Calcular confian√ßa
            confidence = float(np.max(predictions[0]) * 100)

            # Criar dicion√°rio com todas as predi√ß√µes
            all_predictions = {}
            for i, class_name in enumerate(self.class_names):
                all_predictions[class_name] = float(predictions[0][i] * 100)

            # Calcular tempo de infer√™ncia
            inference_time = time.time() - start_time

            # Adicionar ao monitor de performance
            performance_monitor.add_prediction(predicted_class, confidence, inference_time)

            # Log m√©tricas no MLFlow
            mlflow_manager.log_metrics({
                "prediction_inference_time_ms": inference_time * 1000,
                "prediction_confidence": confidence,
                "prediction_count": 1
            })

            # Log distribui√ß√£o de probabilidades
            prob_metrics = {f"prob_{class_name}": prob for class_name, prob in all_predictions.items()}
            mlflow_manager.log_metrics(prob_metrics)

            logger.info(f"Prediction: {predicted_class} with {confidence:.2f}% confidence (inference: {inference_time:.3f}s)")

            return predicted_class, confidence, all_predictions

        except Exception as e:
            inference_time = time.time() - start_time

            # Log erro no MLFlow
            mlflow_manager.log_metrics({
                "prediction_error_count": 1,
                "prediction_error_time_ms": inference_time * 1000
            })

            mlflow_manager.log_params({
                "prediction_error": str(e)
            })

            logger.error(f"‚ùå Error making prediction: {str(e)}")
            raise
    
    def is_model_loaded(self) -> bool:
        """Verifica se o modelo est√° carregado"""
        return self.is_loaded

# Inst√¢ncia global do servi√ßo
ml_service = MLService()
